---
title: "Algae Blooms"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: '2022-03-30'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data wrangling and visualization

```{r }
library(DMwR)
head(algae)

```

```{r , echo=FALSE}
summary(algae)

```

```{r}
hist(algae$mxPH,prob=T)

```

```{r,echo=FALSE}
library(car)
par(mfrow=c(1,2))
hist(algae$mxPH, prob=T,xlab = "", main = 'Histogram of maximum pH value',ylim = 0:1)
#lines(density(algae$mxPH),na.omit=T)
rug(jitter(algae$mxPH))
qqPlot(algae$mxPH,main = 'qqplot of the algae dataset')
par(mfrow=c(1,1))

```

Here, we can say that. The values on the MxPH is normally distributed with 95% confidence interval. with some values breaking the interval. The value number 56,57 can be termed as the outliers.

```{r}
library(lattice)
```


```{r}
boxplot(size~a1, data = algae,ylab = "River size",xlab="algal A1")

```
```{r}
library(Hmisc)
bwplot(size~a1,data = algae, panel = panel.bwplot,probs=seq(0.01,.49,by=0.01),datadensity=TRUE,
       ylab = "river size",xlab = 'algae A1')
```
```{r}
manyNAs(algae,0.2)


```

```{r}
#romoving the coulums of the dataset
algae <- algae[-manyNAs(algae),]
#cleaning the dataset and finding out the mean of it
clean.algae <- knnImputation(algae,k=10,meth = "median")

```

```{r}
#finding out the right model for the dataset
lm_A <- lm(a1 ~. ,data = clean.algae[,4:18])
summary(lm_A)
```

```{r}
#finding out the right model for the dataset
lm_A <- lm(a1 ~. ,data = clean.algae[,1:12])
summary(lm_A)

```
#important thing to note here is that in R the predictor with 3 categorical variable is set to dummar variable in R.R will create a set of auxiliary variable or other wise called as the dummy variable. Namely,for each factor variable have the values 0 or 1.

```{r}
plot(lm_A)

```
```{r}
anova(lm_A)
```

Here we can see that the season variable is of no use and not relevant to the model , so we will update the model by removing the season variable form it.

```{r}
lm_B <- update(lm_A,.~.-season)#runnnig the model by removing the season variable form it
summary(lm_B)
```
the fit has improved but not improved that much.

```{r}
#checking the difference between both the model and finding out the best model form it
anova(lm_A,lm_B)

```

the sum of sq difference has decreased by -449.8 the comparison shows that the differnce are not significant (a value of 0.6952)
suggest that we have only 30% CONFIDENCE to say that they are different. still we should recall that the new model is simpler.

#now in order to see if we can remove some more elements from our model, we will going to use the backward elemination method.

```{r}
final.lm <- step(lm_A)

```

the function step uses the Akaike Information criterion to perform the model search. The search uses the backward elimination by default.

```{r}
summary(final.lm)
```

#so with this we can conclude that the the proportion of variance explained by this model is still not very interesting. This kind of proportion is usually considered a sign that the linearity assumption of this model are inadequate for the domain.

##Now we are going to use the Regression Trees

```{r}
library(rpart)
rt_A <- rpart(a1 ~., data= algae[,1:12])
#finding the summary of the model
rt_A

```

```{r}
#plotting the regression tree of the model
prettyTree(rt_A)
```
#the function rpart uses the to obtain the tree only grows the tree, stopping when certain criteria are met. Namely, the tree stops growing whenever (1)the decrease in deviance goes below certain threshold (2)the number of samples in the node isless than another threshold (3)the tree depth exceeds another value. There are certain threshold that are already defualt in the mode those are cp,minsplit and maxdepth, and their values are 0.01,20,30 respectively.

```{r}
printcp(rt_A)
```

#in the above summary a k-10 fold cross validation is performed and the error and the standard error are obtained in it.

```{r}
rt_B <- prune(rt_A,cp=0.8)#WE NEED to choose the best tree according to the 1-se rule.
rt_B

```
```{r}
(rt_A <- rpartXse(a1~.,data = algae[,1:12]))#in this package it automates this process and takes as argument the se value, defaulting to 1.

```


r also allows a kind of interactive pruning of a tree through the function snip.rpart(). This function can be used to generate a pruned tree in two ways.

```{r}
first.tree <- rpart(a1~.,data = algae[,1:12])
snip.rpart(first.tree,c(4,7))
```
```{r}
prettyTree(first.tree)
```


```{r}
my.tree <- snip.rpart(first.tree,c(4,7))
my.tree


```
#the rpart package implements a pruning method called the cost complexity pruning.This mehtod uses the values of the parameter cp and R calculates for each node of the tree. The pruning method tries to estimate the value of cp that ensures the best compromise between predictive accuracy and tree size.


#NEXT step is to perform the cross validation find out the results of it
the predictive performance of the regression model is obtained by comparing the prediction of model with the real target variables,
and calcualting some mean absolute error (MAE)

```{r}

lm.prediction.A <- predict(final.lm,clean.algae)#using the predict function on the final model to the original dataset
rt.prediction.A <- predict(rt_A,algae)#this is the way of finding the prediction values for both the model 

```

#after finding the prediction values we are going to calcualte the mean value of the whole prediction

```{r}
(mae.a1.lm <- mean(abs(lm.prediction.A - algae[,"a1"])))#so here we got the mean squared error in a single 

```

```{r}
(mae.a1.rt <- mean(abs(rt.prediction.A - algae[,"a1"])))#so here we got the value form the regression model that we used
```
#another popular method is the mean squared error, lets calculate it

```{r}
(mae.a1.lm <- mean((lm.prediction.A - algae[,"a1"])^2))#hurray we got the value
```

```{r}
(mae.a1.rt <- mean(abs(rt.prediction.A - algae[,"a1"])^2))#and we got the value for both the model
```

#so the MSE has the disadvantage of not being meased in the same units as the target variable.

#now we will be using the mean squared erroe and finding out the value of it 

```{r}

(nmse.a1.lm<-  mean((lm.prediction.A - algae[,'a1'])^2) /mean((mean(algae[,'a1'])-algae[,'a1'])^2)) #ohh finally after 10 minutes of debugging it finally ran and provided us the results 

```
#doing the same for the regression tree
```{r}
(nmse.a1.rt<-  mean((rt.prediction.A - algae[,'a1'])^2) /mean((mean(algae[,'a1'])-algae[,'a1'])^2))
```
how to interpret the value of nmse , so its value ranges form 0 to 1 , the lower the value of nmse the better it is.values greater than 1 mean that your model is perfoming worse than simply predicting the average of all cases!

#using another r function for the above task
```{r}
regr.eval(algae[,"a1"],rt.prediction.A,train.y = algae[,"a1"])
```
In this next step, we are going to evaluate our model in the basis of k-fold cross validation process. 
In general when we are facing a predictive task we have to make the following decision.

1)select the alternative model to consider
2)select the evaluation metrics that will be used to compare the models.
3)choose the experimental methodology for obtaining reliable estimates of these matrics.

#let's construct the functions
```{r}
cv.rpart <- function(form,train,test,...){
  m <- rpartXse(form,train,...)
  p <- predict(m,test)
  mse <- mean((p-resp(form,test))^2)
  c(nmse=mse/mean((mean(resp(form,train))-resp(form,test))^2))
}

```


In this illustrarive example we have assumed that we want to use the NMSE as evaluation metric of our regression tree and linear model.

```{r}
cv.lm <- function(form,train,test,...){
  m <- lm(form,train,...)
  p <- predict(m,test)
  p <- ifelse(p<0,0,p)
  mse <- mean((p-resp(form,test))^2)
  c(nmse=mse/mean((mean(resp(form,train))-resp(form,test))^2))
}

```

Having defined my functions that will carry out the learning and testing phase of our model, we can carry out the cross validation comparison as follows:


In this illustrative example, we have assumed that we want to use the NMSE as evaluation metric of our regression trees and linear model. All of the user defined functions should have as the first three parameter(a formula),(a training data),(a test data). The remaining parameter that may be included in the call by the experimental routines are parameters of learner being evaluate .Both parameter carry out the same test + train + evaluation cycle although using obviously a different learing algorithm. Both return as result a named vector with the score in terms of NMSE.
```{r}
res<-experimentalComparison(dataset(a1~.,clean.algae[,1:12],'a1'),c(variants('cv.lm',se=c(0,0.5,1))
                            ,c(variants('cv.rpart',se=c(0,0.5,1)))), cvSettings(3,10,1234)
)

```
as mentioned previously, the first argument should be a vector with the datasets to ve used in the experimental comparison.
Each dataset is specified as dataset(<formula>,<dataframe>,<label>). the secound argument of experimentalComparison() contains a vector of learining systems varients.Each varients is specified using the function variants().It's first argument is the name of user defined function that will carry out the learn+test+evaluate cycle.



```{r}
summary(res)
```


As, we have seen one of the variant of regression tree has got the lowest possible score .whether the difference is statistically significant with respect to the other alternatives is a question we will address later in this section:

```{r}
plot(res)
```
#visualiztion of cross validation result



```{r}
getVariant("cv.rpart.v1",res)
```
Now we are going to carry out the similar comparative experiment for all seven predictors tasks we are facing at the same time. the following code implements that

```{r}
DSs <- sapply(names(clean.algae)[12:18], function(x,names.attrs)
{
  f <- as.formula(paste(x,"~."))
  dataset(f,clean.algae[,c(names.attrs,x)],x)
},
  names(clean.algae)[1:11])

```

#it ran successfully
```{r}
res.all <- experimentalComparison(
  DSs,
  c(variants('cv.lm'),
    variants('cv.rpart',se=c(0,0.5,1))
  ),
  cvSettings(5,10,1234)
  
  
)
```
now, we will plot the result of it and check out the comparison in it.

```{r}
plot(res.all)

```
as we can observe there are several very bad results : that is, NMSE scores are clealy above 1, which is the baseline of being competitive as predicting always the average target.If we want to check the best Modal for each problem , we can use the function bestScores(),from our package:

```{r}
bestScores(res.all)
```
the output of this function confirms that, with the exception of algea1, results are rather disappointing. The variability of the results provides us the good indication that this might be a good candidate for an ensemble approach. Ensemble are model
contruction method that basically try to overcome some limitations of individuals model by generating a large set of alternative models and then combining their prediction.


#Random forests are regarded as one of the most competitive example fo ensemble. They are formally the large set of tree-based models.(regression and classification)

Each tree is fully grown (no-post pruning);and at each step of the tree growning process, the best split for each node is chosen from a random subset of attributes.

```{r}
library(randomForest)

cv.rf <- function(form,train,test,...){
  m <- randomForest(form,train,...)
  p <- predict(m,test)
  mse <- mean((p-resp(form,test))^2)
  c(nmse=mse/mean((mean(resp(form,train))-resp(form,test))^2))
}
```

and voila , it worked flawlessly

```{r}
res.all <- experimentalComparison(
  DSs,
  c(variants('cv.lm'),
    variants('cv.rpart',se=c(0,0.5,1)),
    variants('cv.rf',ntree=c(200,500,700))
    ),
  cvSettings(5,10,1234))

```

#finding out the best score of the model we can get !

```{r}
bestScores(res.all)
```
As we can see the "cv.rf.v3" is the best model for out dataset, and also for the algae 1,2,4 and 6 :

```{r}
compAnalysis(res.all,against = 'cv.rf.v3',
             datasets = c('a1','a2','a4','a6'))

```

#prediction for the seven algae
the following code obtain all 7 models

```{r}
bestModelsNames <- sapply(bestScores(res.all),
                          function(x) x['nmse','system'])
learners <- c(rf='randomForest',rpart='rpartXse')
funcs <- learners[sapply(strsplit(bestModelsNames,'\\.'),
                         function(x)x[2])]
parSetts <- lapply(bestModelsNames,
                   function(x) getVariant(x,res.all)@pars)
bestModels <- list()
for(a in 1:7){
  form <- as.formula(paste(names(clean.algae)[11+a],'~.'))
        bestModels[[a]]<- do.call(funcs[a],
              c(list(form,clean.algae[,c(1:11,11+a)]),parSetts[[a]]))
                           }

```
#hard to know why the function is not running

```{r}
clean.test.algae <- knnImputation(test.algae,k=10,distData = algae[,1,11])

```
#still the above code does not work but will check later

```{r}
preds <- matrix(ncol = 7,nrow = 140)
for (i in 1:nrow(clean.test.algae))
  preds[i,] <- sapply(1:7, function(x)
    predict(bestModels[[x]],clean.test.algae[i,])
    )

```

#with the above code we can obtain a matrix(preds),with the required 7*140 prediction. At this stage we can compare these predictions with the real values to obtain some prediction and on the quality of approach to this prediction problem.

```{r}
avg.preds <- apply(algae[,12:18],2,mean)
apply(((algae.sols-preds)^2),2,mean)/apply((scale(algae.sols,avg.preds,F)^2),2,mean)

```


#summary
In terms of mining the case study has provided us -
1) Data visualization
2) Descriptive Statistics
3) Strategies to handle the missing data
4) Regression tasks
5) Evaluation metrics for regression tasks
6) Multiple linear regression
7) Regression trees
8) Model selection/comparison through k-fold cross-validation
9) Model ensamble and random forests


